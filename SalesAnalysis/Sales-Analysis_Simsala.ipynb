{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a39441",
   "metadata": {},
   "source": [
    "# Sales Analysis with Pandas\n",
    "\n",
    "This project is inspired by the work of Keith Galli, available on his GitHub repository: Pandas Data Science Tasks (https://github.com/KeithGalli/Pandas-Data-Science-Tasks). Keith presents a series of intriguing questions related to a dataset, providing an excellent opportunity to practice our skills in data manipulation and analysis using Python's Pandas and Matplotlib libraries.\n",
    "\n",
    "## Key Challenges\n",
    "\n",
    "Throughout this project, we will tackle the following challenges:\n",
    "\n",
    "- Best Month for Sales and Earnings: We will determine the best-performing month in terms of sales and calculate the total earnings for that specific month.\n",
    "- Top-Selling City: Identifying the city that generated the highest sales volume in terms of ordered quantity.\n",
    "- Optimal Advertisement Timing: Investigating the optimal time for displaying advertisements to maximize the likelihood of customer purchases.\n",
    "- Frequently Purchased Products: Discovering which products are often purchased together.\n",
    "- Best-Selling Product: Identifying the product that achieved the highest sales, and offering insights into the factors contributing to its success.\n",
    "\n",
    "To address these questions, we will leverage the power of Python's Pandas and Matplotlib libraries. \n",
    "\n",
    "## Project Approach\n",
    "\n",
    "Rather than simply following the tutorial, this project encourages active engagement and problem-solving. I will independently work through each question, utilizing Pandas for data manipulation and analysis and Matplotlib for data visualization. My goal is not only to answer these questions but also to understand the underlying techniques and concepts involved in the process.\n",
    "\n",
    "By the end of this project, I aim to have a deeper understanding of data science techniques and gain more confidence in using Pandas and Matplotlib for real-world data analysis tasks.\n",
    "\n",
    "Let's embark on this data-driven journey, exploring the dataset and uncovering valuable insights along the way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919104f",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "### Imports\n",
    "First of all we will do all necessary imports for the project. With the OS library (Miscellaneous operating system interfaces) we have a module that provides us with a portable way of using operating system dependent functionalities. With the panads library, we can do fast, powerful, flexible and easy data analysis and manipulation. We will need the numpy library for some calculations to create x- & y-grids to plot a 2D Kernel Estimate for better understanding of the data. For plotting reasons we will use the matplotlib library. We will also need the gaussian_kde library for our Plot based on the 2D Kernel Estimates. Furthermore, we need for question four the functions combinations and counter to identify products that have been ordered together. Finally, to check for significant correlations we will use the pearsonr function from the scipy.stats library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b5e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9cc58",
   "metadata": {},
   "source": [
    "### Read and Merge Sales Data of the whole Year\n",
    "First of all we have to read in the sales data. The sales data is contained in multiple CSV files within a fodler in the working diretory. In the following we will read all the csv files, concatenate them and write the output to another CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42199da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of folder where data is in\n",
    "path = \"./Sales_Data\"\n",
    "\n",
    "# save name of csv files in list\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "# Create Dataframe, read csv files and save them into a dataframe\n",
    "dfs = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    current_data = pd.read_csv(path+\"/\"+csv)\n",
    "    dfs.append(current_data)\n",
    "\n",
    "combined_df = pd.concat(dfs)\n",
    "\n",
    "# Next, we save the dataframe in a new csv file, without the index column:\n",
    "combined_df.to_csv(\"./Output/Sales_Data_2019\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fc40af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Output/Sales_Data_2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#combined_df.to_csv(\"Sales_Data_2019.csv\", index=False)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Output/Sales_Data_2019.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DaSi\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DaSi\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DaSi\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DaSi\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DaSi\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Output/Sales_Data_2019.csv'"
     ]
    }
   ],
   "source": [
    "#combined_df.to_csv(\"Sales_Data_2019.csv\", index=False)\n",
    "df = pd.read_csv(\"./Output/Sales_Data_2019.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf995a1a",
   "metadata": {},
   "source": [
    "###  Observe, Clean, and Prepare Data\n",
    "\n",
    "First we will observe the data, its structure, and its current statistics. This will help us to get a basic understanding of how the data looks like and if there might be some issues with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7283ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first couple rows to get an idea of the format of each column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0171da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a2e84",
   "metadata": {},
   "source": [
    "Accidently, we concatenated not just the data but also the headlines of each table. Thus, we will remove the rows that contain the entry \"Order ID\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda701a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['Order ID'] == \"Order ID\"].index)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929f136",
   "metadata": {},
   "source": [
    "#### Handling Unknown Values\n",
    "Now let us have a closer look at the Na's, where they occur and how we should treat these unknown values. Let us therefore first see how many NAs we have in our Dataframe. Next we will check how many NAs we have in each Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unknown values exist within the whole dataframe\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b9c64",
   "metadata": {},
   "source": [
    "There are plenty of unknown values within our DataFrame. Let us have a closer look in which columne these values actually accure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "# iterating the columns\n",
    "for col in df.columns:\n",
    "    col_names += [col]\n",
    "\n",
    "#Count nas across the columns\n",
    "nas_list = []\n",
    "i=0\n",
    "# iterating the columns\n",
    "for col in df.columns:\n",
    "    nas = df[col_names[i]].isna().sum()\n",
    "    nas_list.append(nas)\n",
    "    i += 1\n",
    "    \n",
    "na_count = pd.DataFrame([nas_list],[\"Number of Nas\"])\n",
    "na_count.columns = [col_names]\n",
    "na_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e7afa",
   "metadata": {},
   "source": [
    "We have plenty of NAs in the Dataframe. Each column has the same amount of NAs. This might indicate that we have full rows of NAs. After checking the csv files, it appears to be that all NAs are always in the same row. Thus, we can just remove all rows that contain NA values, without the fear of losing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a252cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, how=\"all\")\n",
    "# Check once more how many unknown values exist within the whole dataframe\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de70c8",
   "metadata": {},
   "source": [
    "### Create New Columns for Day, Month, Year and Time\n",
    "As we want analyze the data based on different levels of time, we will add more columns, which contain information about the day, month and year of the order. We will additionaly parse the columns for day, month, and year into integers as it makes sense to handle this values as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Order Date'] = pd.to_datetime(df['Order Date'],format=\"%m/%d/%y %H:%M\")\n",
    "df['Month'] = df['Order Date'].dt.month\n",
    "df['Day'] = df['Order Date'].dt.day\n",
    "df['Year'] = df['Order Date'].dt.year\n",
    "df['Time'] = df['Order Date'].dt.time\n",
    "df['Hour'] = df['Order Date'].dt.hour\n",
    "df['Minute'] = df['Order Date'].dt.minute\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d264c1",
   "metadata": {},
   "source": [
    "### Create Street, City, State, and ZIP Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Street\",\"City\",\"State\"]] = df['Purchase Address'].str.split(',', expand=True)\n",
    "df['ZIP'] = df['State'].str[3:]\n",
    "df['State'] = df['State'].str[1:3]\n",
    "df['City'] = df['City']+\" (\"+df['State']+\")\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd90a0b",
   "metadata": {},
   "source": [
    "### Create a Sales Column\n",
    "In addition, we want to know more about the Sales numbers. Therefore, we will calculate the sales of each order by multiplyng the quantity with the price for each item. As the values wihtin these columns are non numerical values, we have to parse from string to float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse to numeric values for Quantity & Price \n",
    "df[\"Quantity Ordered\"] = df[\"Quantity Ordered\"].astype('float')\n",
    "df[\"Price Each\"] = df[\"Price Each\"].astype('float')\n",
    "df['Sales'] = df[\"Quantity Ordered\"]*df[\"Price Each\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773fb9c",
   "metadata": {},
   "source": [
    "## Solving the Challenges\n",
    "### Question 1: What was the best month for sales? How much was earned that month?\n",
    "\n",
    "This question is straightforward and easy to solve with a couple of lines of code. We solely have to sum up each months sales. To better see how the sales have been distributed across the year, we can vizualize each months sales within one bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7519d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of all Sales based on Months\n",
    "Monthly_Sales = df.groupby(['Month'])['Sales'].sum()\n",
    "# Parse data again into a DataFrame\n",
    "Monthly_Sales = pd.DataFrame(Monthly_Sales)\n",
    "# Print Monthly Sales by ordered in descending order by Best Selling Months \n",
    "print(Monthly_Sales.sort_values(by=\"Sales\",ascending=False))\n",
    "# Create month column fpr vizualizing purposes\n",
    "Monthly_Sales['Month'] = Monthly_Sales.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d27d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure\n",
    "fig= plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create a bar plot for monthly sales\n",
    "plt.bar(Monthly_Sales[\"Month\"], Monthly_Sales[\"Sales\"])\n",
    "\n",
    "# Set plot title, labels, and x-axis ticks\n",
    "plt.title(\"Monthly Sales in 2019\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.xticks(Monthly_Sales[\"Month\"])  # Use the \"Month\" values for x-axis ticks\n",
    "\n",
    "# Set y-axis label and customize y-axis tick values\n",
    "plt.ylabel(\"Sales in USD\")\n",
    "y_tick_values = [1000000, 2000000, 3000000, 4000000]\n",
    "plt.yticks(y_tick_values)\n",
    "\n",
    "# Format y-axis tick labels with commas for better readability\n",
    "current_values = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels(['{:,.0f}'.format(x) for x in current_values])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dba675",
   "metadata": {},
   "source": [
    "Answer to Question 1: We can clearly see that december is the month with the highest sales. This might be due to the christmas holidays where people buy presents for their loved ones. Furthermore, people might get bonuses towards the end of the year and have more money available than troughtout the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68007bb6",
   "metadata": {},
   "source": [
    "### Question 2: What city sold the most product?\n",
    "\n",
    "This question is a bit more tricky as we there might be multiple cities with the same name. Therefore, we have to check the State of each city to make sure we do not misinterpret the data. In the following we will summarize the ordered quantity of each city from the whole year. We are taking into account that cities can have the same name (e.g. Portland), but different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of all Sales based on Months\n",
    "City_Quantities = df.groupby(['City'])['Quantity Ordered'].sum()\n",
    "City_Quantities = pd.DataFrame(City_Quantities)\n",
    "# Print City_Quantities by ordered in descending order by most sold products\n",
    "print(City_Quantities.sort_values(by=\"Quantity Ordered\",ascending=False))\n",
    "City_Quantities['City'] = City_Quantities.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig= plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort the data by \"Quantity Ordered\" in ascending order\n",
    "City_Quantities = City_Quantities.sort_values(by=\"Quantity Ordered\", ascending=True)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(City_Quantities[\"City\"], City_Quantities[\"Quantity Ordered\"])\n",
    "\n",
    "# Set plot title, labels, and customize tick rotation\n",
    "plt.title(\"Quantities Sold in 2019\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.xticks(City_Quantities[\"City\"], rotation='vertical')\n",
    "plt.ylabel(\"Quantities\")\n",
    "\n",
    "# Customize y-axis tick values and formatting\n",
    "plt.yticks([10000, 20000, 30000, 40000, 50000, 60000])\n",
    "current_values = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels(['{:,.0f}'.format(x) for x in current_values])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb187ef9",
   "metadata": {},
   "source": [
    "### Question 3: What time should we display advertisemens to maximize the likelihood of customer’s buying product?\n",
    "\n",
    "This question is not very intuitive, as it is very difficult to measure the influence of and displayed advertisment and how this advertisment affects the likelihood of customer's buying a product. Furthermore, we do not have usage data of the website, on which the buyer is placing it's order and the advertisment channels where the customer receives the ads from. But what we have is the time when each order was placed. This gives us an indication when the buyers might be most likely to buy the product. Thus, we will have a look at the number of orders across each hour throughout the day. Therefore, we have to count the orders based on the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of Orders per occured time entry on hour level\n",
    "Orders_by_H = pd.DataFrame()\n",
    "Orders_by_H[\"Number of Orders\"] = df.groupby([\"Hour\"])['Order ID'].count()\n",
    "Orders_by_H=Orders_by_H.reset_index()\n",
    "Orders_by_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a line plot\n",
    "\n",
    "# Set the figure size (width, height)\n",
    "plt.figure(figsize=(12, 6))  \n",
    "# Create the line plot\n",
    "plt.plot(Orders_by_H['Hour'], Orders_by_H['Number of Orders'], marker='o', linestyle='-')  \n",
    "# Labels for x & y axis + Title of the plot + setting tick lockation + adding grid lines to the plot\n",
    "plt.xlabel('Hour of the Day') \n",
    "plt.ylabel('Number of Orders') \n",
    "plt.title('Number of Orders by Hour of the Day') \n",
    "plt.xticks(Orders_by_H['Hour']) \n",
    "plt.grid(True, linestyle='--', alpha=0.7) \n",
    "\n",
    "plt.tight_layout()  # Ensure labels and title fit within the figure\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6de08",
   "metadata": {},
   "source": [
    "Based on this first analysis we can already see, that the customers place more orders during certain times throughout the day. Thus, we could conclude, that we should place advertisment during or even before the peak times of the placed number of orders. The drawback here is that we just see the numbers of orders per hour. Thus, orders placed close to 11.00 am are still counted towards 10 am. As advertisments are very expensive, it might make sense to have a more detailed temporal view. In the next two plots we will have a look at the number of ordes on a minute level. We will also look at the 2D-Kernel Density Plot to get a better understanding of the distributions of time and orders throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of Orders per occured time entry on minute level\n",
    "Orders_by_Time = pd.DataFrame()\n",
    "Orders_by_Time[\"Number of Orders\"] = df.groupby(['Time'])['Order ID'].count()\n",
    "Orders_by_Time=Orders_by_Time.reset_index()\n",
    "Orders_by_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f71e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Time' column to string\n",
    "Orders_by_Time['Time'] = Orders_by_Time['Time'].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the data\n",
    "ax.scatter(Orders_by_Time['Time'], Orders_by_Time['Number of Orders'], marker='o', linestyle='-')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Time of Day')\n",
    "ax.set_ylabel('Number of Orders')\n",
    "ax.set_title('Number of Orders vs. Time of Day')\n",
    "ax.grid(True)\n",
    "\n",
    "# Set custom x-axis ticks for every 2 hours\n",
    "x_ticks = Orders_by_Time['Time'][::60]  # Assuming data is recorded every minute (60 minutes x 2)\n",
    "plt.xticks(x_ticks, rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensures labels and titles fit within the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f10401",
   "metadata": {},
   "source": [
    "Answer Question 3: The previous plot gives us already great insights regarding the times in which the orders are coming in. This plot represents a scatter plot that shows how many orders appeared each minute throughout a day (based on data of the whole year). We can see, that the peak times for incoming orders are between 11.00-13.00 o'clock and between 19.00-21.00 o'clock. Thus, we can try to improve the likelihood for customers to buy products by advertising our products during these peak order times. Obviously, there are a variety of different products, so further analysis is necessary to identify the product that we actually should advertise during which times. But for now, this analysis is sufficient to answer question 3. \n",
    "\n",
    "In the next section there is a 2D-Kernel Density Plot for minutes and the number of orders. This plot was just for plotting practice puposes and didn't help further in answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Time' column to datetime\n",
    "Orders_by_Time['Time'] = pd.to_datetime(Orders_by_Time['Time'], format='%H:%M:%S')\n",
    "\n",
    "# Extract minutes from datetime\n",
    "Orders_by_Time['Time (minutes)'] = Orders_by_Time['Time'].dt.hour * 60 + Orders_by_Time['Time'].dt.minute\n",
    "\n",
    "# Create a 2D kernel density estimate\n",
    "x = Orders_by_Time['Time (minutes)']\n",
    "y = Orders_by_Time['Number of Orders']\n",
    "xy = np.vstack([x, y])\n",
    "k = gaussian_kde(xy)\n",
    "\n",
    "# Define the grid for the plot\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(x.min(), x.max(), 100), np.linspace(y.min(), y.max(), 100))\n",
    "grid_points = np.vstack([x_grid.ravel(), y_grid.ravel()])\n",
    "z = k(grid_points)\n",
    "\n",
    "# Create a figure and axis with a larger size\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "contour = ax.contourf(x_grid, y_grid, z.reshape(x_grid.shape), cmap='viridis')\n",
    "\n",
    "# Customize the plot (add labels and a title)\n",
    "plt.xlabel('Time (minutes since midnight)')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.title('2D Kernel Density Estimate')\n",
    "\n",
    "# Create a colorbar manually\n",
    "cbar = plt.colorbar(contour)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15debc40",
   "metadata": {},
   "source": [
    "### Question 4: What products are most often sold together?\n",
    "\n",
    "This question is a very challenging one. To reduce calculation time along the way, we will just look at the Order IDs that are mentioned at least twice in the dataframe. Afterwards, we need to group the products that occur in the same orders. Next we need extra help from the library itertools as we need to get all product combinations for each order. Therefore, we will use the combinations function from the itertool library to get all the combination for the products within each order. As a result we have a new column called \"Product Combinations\". This column contains all different product combinations. It is important to mention that the combinations function won't count in both orders of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c100a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first remove all non duplicates to save calculation power along the way\n",
    "relevant_columns = df[df['Order ID'].duplicated(keep=False)]\n",
    "\n",
    "# Create a new DataFrame with relevant columns\n",
    "relevant_columns = relevant_columns[['Order ID', 'Product']]\n",
    "\n",
    "# Sort Orders and columns\n",
    "relevant_columns = relevant_columns.sort_values(['Order ID','Product'])\n",
    "\n",
    "# Group data by order ID and create lists of products in each order\n",
    "orders_grouped = relevant_columns.groupby('Order ID')['Product'].apply(list).reset_index()\n",
    "\n",
    "# Create a function to find product combinations\n",
    "def find_product_combinations(products_list):\n",
    "    product_combinations = list(combinations(products_list, 2))\n",
    "    return product_combinations\n",
    "\n",
    "# Apply the function to each row to find product combinations\n",
    "orders_grouped['Product Combinations'] = orders_grouped['Product'].apply(find_product_combinations)\n",
    "\n",
    "# Flatten the combinations\n",
    "product_combinations_flat = [item for sublist in orders_grouped['Product Combinations'] for item in sublist]\n",
    "\n",
    "# Count the occurrences of each product combination\n",
    "product_combination_counts = Counter(product_combinations_flat)\n",
    "\n",
    "# Find the most frequently sold product combinations\n",
    "most_common_combinations = product_combination_counts.most_common(15)\n",
    "\n",
    "# Save it as dataframe and print results\n",
    "most_common_combinations_df = pd.DataFrame(most_common_combinations)\n",
    "most_common_combinations_df = most_common_combinations_df.rename(columns={0: \"Product Combination\", 1: \"Count\"})\n",
    "most_common_combinations_df['Product Combination'] = most_common_combinations_df['Product Combination'].apply(lambda x: ' & '.join(map(str, x)))\n",
    "most_common_combinations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure \n",
    "fig= plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(most_common_combinations_df[\"Product Combination\"], most_common_combinations_df[\"Count\"])\n",
    "\n",
    "# Set plot title, labels, and customize tick rotation\n",
    "plt.title(\"Product Combinations that have been ordered most often together\")\n",
    "plt.xlabel(\"Product Combinations\")\n",
    "plt.xticks(most_common_combinations_df[\"Product Combination\"], rotation='vertical')\n",
    "plt.ylabel(\"Number of times ordered together\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd833a9",
   "metadata": {},
   "source": [
    "#### Answer Question 4: \n",
    "Ultimately, we can say that the **Lightning Charging Cable and the Iphone** have been sold most often together, closely followed by the combination of the **Google Phone and the USB-C Charging Cable**. Knowing what products are most often sold together can provide several benefits for our company, both in terms of improving customer experience and increasing revenue. By identifying pairs or groups of products that are frequently bought together, a seller can create **targeted cross-selling strategies**. For example, if customers often buy a phone and a charging cable together, we can bundle these items or offer discounts when purchased together, increasing the average order value. In addtion, we can use this information to make **personalized product recommendations to customers**. When a customer adds one item to their cart, we can suggest other frequently co-purchased items, enhancing the shopping experience and potentially leading to more sales. These are just some of the benefits of knowing the product combinations of ordered products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94c2d5",
   "metadata": {},
   "source": [
    "### Additonal Analysis of Product combination of three Products\n",
    "\n",
    "In this section we will do the same analysis for combinations with three products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by order ID and create lists of products in each order\n",
    "orders_grouped = relevant_columns.groupby('Order ID')['Product'].apply(list).reset_index()\n",
    "\n",
    "# Create a function to find product combinations\n",
    "def find_product_combinations(products_list):\n",
    "    product_combinations = list(combinations(products_list, 3))\n",
    "    return product_combinations\n",
    "\n",
    "# Apply the function to each row to find product combinations\n",
    "orders_grouped['Product Combinations'] = orders_grouped['Product'].apply(find_product_combinations)\n",
    "\n",
    "# Flatten the combinations\n",
    "product_combinations_flat = [item for sublist in orders_grouped['Product Combinations'] for item in sublist]\n",
    "\n",
    "# Count the occurrences of each product combination\n",
    "product_combination_counts = Counter(product_combinations_flat)\n",
    "\n",
    "# Find the most frequently sold product combinations\n",
    "most_common_combinations = product_combination_counts.most_common(15)\n",
    "\n",
    "# Save it as dataframe and print results\n",
    "most_common_combinations_df = pd.DataFrame(most_common_combinations)\n",
    "most_common_combinations_df = most_common_combinations_df.rename(columns={0: \"Product Combination\", 1: \"Count\"})\n",
    "most_common_combinations_df['Product Combination'] = most_common_combinations_df['Product Combination'].apply(lambda x: ' & '.join(map(str, x)))\n",
    "most_common_combinations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure \n",
    "fig= plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(most_common_combinations_df[\"Product Combination\"], most_common_combinations_df[\"Count\"])\n",
    "\n",
    "# Set plot title, labels, and customize tick rotation\n",
    "plt.title(\"Product Combinations of three Items that have been ordered most often together\")\n",
    "plt.xlabel(\"Product Combinations\")\n",
    "plt.xticks(most_common_combinations_df[\"Product Combination\"], rotation='vertical')\n",
    "plt.ylabel(\"Number of times ordered together\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3edf59",
   "metadata": {},
   "source": [
    "Ultimately, we can say that the **Google Phone, the USB-C Charging Cable and Wired Headphones** have been sold most often together, closely followed by the combination of the **Lightning Charging Cable, the Iphone, and Wired Headphones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e2c88",
   "metadata": {},
   "source": [
    "### Question 5: What product sold the most? Why do you think it sold the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of all Sales based on Months\n",
    "Product_Quantities = df.groupby(['Product'])['Quantity Ordered'].sum()\n",
    "Product_Quantities = pd.DataFrame(Product_Quantities)\n",
    "Product_Quantities = Product_Quantities.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig= plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort the data by \"Quantity Ordered\" in ascending order\n",
    "Product_Quantities = Product_Quantities.sort_values(by=\"Quantity Ordered\", ascending=True)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(Product_Quantities[\"Product\"], Product_Quantities[\"Quantity Ordered\"])\n",
    "\n",
    "# Set plot title, labels, and customize tick rotation\n",
    "plt.title(\"Quantities Sold in 2019\")\n",
    "plt.xlabel(\"Products\")\n",
    "plt.xticks(Product_Quantities[\"Product\"], rotation='vertical')\n",
    "plt.ylabel(\"Quantities\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea6b0a",
   "metadata": {},
   "source": [
    "As we can see, products with a relatively low price are amongst the ones ordered the most (e.g. Batteries, Charging Cables, Headphones). Let us see if there is a correlation between price and quantities and let us create more vizualizations to see the relationship between the prices and the quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df197aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of all Sales based on Months\n",
    "Product_Price = df.groupby(['Product'])['Price Each'].mean()\n",
    "Product_Price = pd.DataFrame(Product_Price)\n",
    "Product_Price = Product_Price.reset_index()\n",
    "\n",
    "Products = pd.merge(Product_Price, Product_Quantities, on=['Product'])\n",
    "Products = Products.sort_values(by=\"Quantity Ordered\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot\n",
    "plt.scatter(Products['Price Each'], Products['Quantity Ordered'])\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Price Each')\n",
    "plt.ylabel('Quantity Ordered')\n",
    "plt.title('Relationship of Price & Quantitiy')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc735f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation correlation coefficient and p-value between x and y\n",
    "pearsonr(Products['Price Each'], Products['Quantity Ordered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the width of each bar group\n",
    "bar_width = 0.35\n",
    "\n",
    "# Create the figure and the first y-axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "products = Products['Product']\n",
    "quantities = Products['Quantity Ordered']\n",
    "prices = Products['Price Each']\n",
    "\n",
    "# Calculate the x positions for each group of bars\n",
    "x_quantities = np.arange(len(products))\n",
    "x_prices = [x + bar_width for x in x_quantities]\n",
    "\n",
    "# Create the first set of bars for quantities\n",
    "ax1.bar(x_quantities, quantities, width=bar_width, color='b', alpha=0.7, label='Quantities')\n",
    "ax1.set_xlabel('Products')\n",
    "ax1.set_ylabel('Quantities', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Create the second y-axis (twin y-axis)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Create the second set of bars for prices\n",
    "ax2.bar(x_prices, prices, width=bar_width, color='r', alpha=0.7, label='Prices')\n",
    "ax2.set_ylabel('Prices in USD', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Set x-axis ticks and labels for each group\n",
    "x_ticks = [x + bar_width / 2 for x in x_quantities]\n",
    "ax1.set_xticks(x_ticks)\n",
    "ax1.set_xticklabels(products, rotation=90)\n",
    "\n",
    "# Adding legends\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set a title\n",
    "plt.title('Comparison of Prices and Quantities by Product')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensures labels and titles fit within the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec397a",
   "metadata": {},
   "source": [
    "#### Answer Question 5:\n",
    "It appears to be that we especially sold many products that have a very low price. More pricey items such as Dryers, Laptops, Monitors and Phones are less freuqent orded as less expsenive ones. Also the pearson correlation test shows a significant negative correlation  (-0.6) of price and quantity. As price might be one reason why batteries, charging cables and headphones are ordered more frequently, there migth be also other reasons. Batteries, charging cables, and headphones are consumable items, meaning they have a limited lifespan, and customers need to replace them regularly. For example, batteries may need to be replaced when they run out of charge, charging cables can wear out or get lost, and headphones may break or wear down over time.\n",
    "Furthermore, these products have versatile uses and are compatible with many devices. Charging cables, for instance, can be used with various electronic devices such as smartphones, tablets, and laptops. This versatility increases the likelihood of customers needing multiple cables. Finally customers may find it more convenient and cost-effective to replace inexpensive items like batteries, charging cables, and headphones rather than investing in costly repairs or replacements of the devices they connect to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
